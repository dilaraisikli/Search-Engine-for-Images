{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4pNC1vgUEc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# You'll generate plots of attention in order to see which parts of an image\n",
        "# our model focuses on during captioning\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Scikit-learn includes many helpful utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "import csv\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76iYutNqGTcc",
        "colab_type": "text"
      },
      "source": [
        "## PART 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HaCvWdn7JY_",
        "colab_type": "text"
      },
      "source": [
        "In this part we are using the code provided that generates the captions. At the end a captions file is created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Lpstcq7UVzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "7e1a7dcf-69a2-4554-8a16-002f347f79db"
      },
      "source": [
        "annotation_folder = '/annotations/'\n",
        "if not os.path.exists(os.path.abspath('.') + annotation_folder):\n",
        "  annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
        "                                          cache_subdir=os.path.abspath('.'),\n",
        "                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip',\n",
        "                                          extract = True)\n",
        "  annotation_file = os.path.dirname(annotation_zip)+'/annotations/captions_train2014.json'\n",
        "  os.remove(annotation_zip)\n",
        "\n",
        "# Download image files\n",
        "image_folder = '/train2014/'\n",
        "if not os.path.exists(os.path.abspath('.') + image_folder):\n",
        "  image_zip = tf.keras.utils.get_file('train2014.zip',\n",
        "                                      cache_subdir=os.path.abspath('.'),\n",
        "                                      origin = 'http://images.cocodataset.org/zips/train2014.zip',\n",
        "                                      extract = True)\n",
        "  PATH = os.path.dirname(image_zip) + image_folder\n",
        "  os.remove(image_zip)\n",
        "else:\n",
        "  PATH = os.path.abspath('.') + image_folder\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "252878848/252872794 [==============================] - 8s 0us/step\n",
            "Downloading data from http://images.cocodataset.org/zips/train2014.zip\n",
            "13510574080/13510573713 [==============================] - 374s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF19Y7D4OLzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "annotation_file = './annotations/captions_train2014.json'\n",
        "PATH = './train2014/'\n",
        "\n",
        "with open(annotation_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "\n",
        "# Store captions and image names in vectors\n",
        "all_captions = []\n",
        "all_img_name_vector = []\n",
        "\n",
        "for annot in annotations['annotations']:\n",
        "    caption = '<start> ' + annot['caption'] + ' <end>'\n",
        "    image_id = annot['image_id']\n",
        "    full_coco_image_path = PATH + 'COCO_train2014_' + '%012d.jpg' % (image_id)\n",
        "\n",
        "    all_img_name_vector.append(full_coco_image_path)\n",
        "    all_captions.append(caption)\n",
        "\n",
        "# Shuffle captions and image_names together\n",
        "# Set a random state\n",
        "\n",
        "train_captions, img_name_vector_jnk = shuffle(all_captions,\n",
        "                                          all_img_name_vector,\n",
        "                                          random_state=1)\n",
        "\n",
        "# Select the first 30000 captions from the shuffled set\n",
        "num_examples = 30000\n",
        "train_captions = train_captions[:num_examples]\n",
        "img_name_vector = img_name_vector_jnk[:num_examples]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwgN4c2vPiSN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f7ea812d-33bc-4df0-d0f5-9d96e1f4baf9"
      },
      "source": [
        " def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    img = tf.keras.applications.inception_v3.preprocess_input(img)\n",
        "    return img, image_path\n",
        "\n",
        "image_model = tf.keras.applications.InceptionV3(include_top=False,\n",
        "                                                weights='imagenet')\n",
        "new_input = image_model.input\n",
        "hidden_layer = image_model.layers[-1].output\n",
        "\n",
        "image_features_extract_model = tf.keras.Model(new_input, hidden_layer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASl0U6Z4UN3v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encode_train = sorted(set(img_name_vector))\n",
        "\n",
        "# Feel free to change batch_size according to your system configuration\n",
        "image_dataset = tf.data.Dataset.from_tensor_slices(encode_train)\n",
        "image_dataset = image_dataset.map(\n",
        "  load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK7Ml00lUPmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for img, path in image_dataset:\n",
        "  batch_features = image_features_extract_model(img)\n",
        "  batch_features = tf.reshape(batch_features,\n",
        "                              (batch_features.shape[0], -1, batch_features.shape[3]))\n",
        "\n",
        "  for bf, p in zip(batch_features, path):\n",
        "    path_of_feature = p.numpy().decode(\"utf-8\")\n",
        "    np.save(path_of_feature, bf.numpy())\n",
        "    #np.save('/content/drive/My Drive/Google TC/embedded_images/'+path_of_feature[12:], bf.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UiqFdeb9eq9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calc_max_length(tensor):\n",
        "    return max(len(t) for t in tensor)\n",
        "\n",
        "top_k = 5000\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n",
        "                                                  oov_token=\"<unk>\",\n",
        "                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^_`{|}~ ')\n",
        "tokenizer.fit_on_texts(train_captions)\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "\n",
        "tokenizer.word_index['<pad>'] = 0\n",
        "tokenizer.index_word[0] = '<pad>'\n",
        "\n",
        "# Create the tokenized vectors\n",
        "train_seqs = tokenizer.texts_to_sequences(train_captions)\n",
        "cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\n",
        "max_length = calc_max_length(train_seqs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mvuNctrDg9zs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_name_train, img_name_val, cap_train, cap_val = train_test_split(img_name_vector,\n",
        "                                                                    cap_vector,\n",
        "                                                                    test_size=0.0333,\n",
        "                                                                    random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bKtqC372TKs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def map_func(img_name, cap):\n",
        "  img_tensor = np.load(img_name.decode('utf-8')+'.npy')\n",
        "  return img_tensor, cap\n",
        "\n",
        "class BahdanauAttention(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units)\n",
        "    self.W2 = tf.keras.layers.Dense(units)\n",
        "    self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "  def call(self, features, hidden):\n",
        "    # features(CNN_encoder output) shape == (batch_size, 64, embedding_dim)\n",
        "\n",
        "    # hidden shape == (batch_size, hidden_size)\n",
        "    # hidden_with_time_axis shape == (batch_size, 1, hidden_size)\n",
        "    hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
        "\n",
        "    # score shape == (batch_size, 64, hidden_size)\n",
        "    score = tf.nn.tanh(self.W1(features) + self.W2(hidden_with_time_axis))\n",
        "\n",
        "    # attention_weights shape == (batch_size, 64, 1)\n",
        "    # you get 1 at the last axis because you are applying score to self.V\n",
        "    attention_weights = tf.nn.softmax(self.V(score), axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * features\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n",
        "class CNN_Encoder(tf.keras.Model):\n",
        "    # Since you have already extracted the features and dumped it using pickle\n",
        "    # This encoder passes those features through a Fully connected layer\n",
        "    def __init__(self, embedding_dim):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        # shape after fc == (batch_size, 64, embedding_dim)\n",
        "        self.fc = tf.keras.layers.Dense(embedding_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.fc(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class RNN_Decoder(tf.keras.Model):\n",
        "  def __init__(self, embedding_dim, units, vocab_size):\n",
        "    super(RNN_Decoder, self).__init__()\n",
        "    self.units = units\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "    self.fc1 = tf.keras.layers.Dense(self.units)\n",
        "    self.fc2 = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = BahdanauAttention(self.units)\n",
        "\n",
        "  def call(self, x, features, hidden):\n",
        "    # defining attention as a separate model\n",
        "    context_vector, attention_weights = self.attention(features, hidden)\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # shape == (batch_size, max_length, hidden_size)\n",
        "    x = self.fc1(output)\n",
        "\n",
        "    # x shape == (batch_size * max_length, hidden_size)\n",
        "    x = tf.reshape(x, (-1, x.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size * max_length, vocab)\n",
        "    x = self.fc2(x)\n",
        "\n",
        "    return x, state, attention_weights\n",
        "\n",
        "  def reset_state(self, batch_size):\n",
        "    return tf.zeros((batch_size, self.units))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKMkm-D2N2hO",
        "colab_type": "text"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLWOokeRNzQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1000\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "vocab_size = top_k + 1\n",
        "num_steps = len(img_name_train) // BATCH_SIZE\n",
        "# Shape of the vector extracted from InceptionV3 is (64, 2048)\n",
        "# These two variables represent that vector shape\n",
        "features_shape = 2048\n",
        "attention_features_shape = 64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VIJyW7rsWS6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((img_name_train, cap_train))\n",
        "\n",
        "# Use map to load the numpy files in parallel\n",
        "dataset = dataset.map(lambda item1, item2: tf.numpy_function(\n",
        "          map_func, [item1, item2], [tf.float32, tf.int32]),\n",
        "          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "# Shuffle and batch\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGW1YOol265g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "encoder = CNN_Encoder(embedding_dim)\n",
        "decoder = RNN_Decoder(embedding_dim, units, vocab_size)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yT3DWNBOGIt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_path = \"/content/drive/My Drive/Google TC/checkpoint\"\n",
        "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
        "                           decoder=decoder,\n",
        "                           optimizer = optimizer)\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "start_epoch = 0\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])\n",
        "  # restoring the latest checkpoint in checkpoint_path\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3YjsGoEOKLl",
        "colab_type": "text"
      },
      "source": [
        "Training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj7b2sLTOMU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_plot = []\n",
        "@tf.function\n",
        "def train_step(img_tensor, target):\n",
        "  loss = 0\n",
        "\n",
        "  # initializing the hidden state for each batch\n",
        "  # because the captions are not related from image to image\n",
        "  hidden = decoder.reset_state(batch_size=target.shape[0])\n",
        "\n",
        "  dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "      features = encoder(img_tensor)\n",
        "\n",
        "      for i in range(1, target.shape[1]):\n",
        "          # passing the features through the decoder\n",
        "          predictions, hidden, _ = decoder(dec_input, features, hidden)\n",
        "\n",
        "          loss += loss_function(target[:, i], predictions)\n",
        "\n",
        "          # using teacher forcing\n",
        "          dec_input = tf.expand_dims(target[:, i], 1)\n",
        "\n",
        "  total_loss = (loss / int(target.shape[1]))\n",
        "\n",
        "  trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, trainable_variables)\n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))\n",
        "\n",
        "  return loss, total_loss\n",
        "\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4SjdNAJOaPF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34dd76d3-bd3c-43ac-fab6-a14a9654801c"
      },
      "source": [
        "EPOCHS = 20\n",
        "\n",
        "for epoch in range(start_epoch, EPOCHS):\n",
        "    start = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (img_tensor, target)) in enumerate(dataset):\n",
        "        batch_loss, t_loss = train_step(img_tensor, target)\n",
        "        total_loss += t_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print ('Epoch {} Batch {} Loss {:.4f}'.format(\n",
        "              epoch + 1, batch, batch_loss.numpy() / int(target.shape[1])))\n",
        "    # storing the epoch end loss value to plot later\n",
        "    loss_plot.append(total_loss / num_steps)\n",
        "\n",
        "    if epoch % 5 == 0:\n",
        "      ckpt_manager.save()\n",
        "\n",
        "    print ('Epoch {} Loss {:.6f}'.format(epoch + 1,\n",
        "                                         total_loss/num_steps))\n",
        "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.0136\n",
            "Epoch 1 Batch 100 Loss 1.2058\n",
            "Epoch 1 Batch 200 Loss 0.9206\n",
            "Epoch 1 Batch 300 Loss 0.9265\n",
            "Epoch 1 Batch 400 Loss 0.8730\n",
            "Epoch 1 Loss 1.004596\n",
            "Time taken for 1 epoch 490.27657866477966 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.8471\n",
            "Epoch 2 Batch 100 Loss 0.8239\n",
            "Epoch 2 Batch 200 Loss 0.7927\n",
            "Epoch 2 Batch 300 Loss 0.7866\n",
            "Epoch 2 Batch 400 Loss 0.8002\n",
            "Epoch 2 Loss 0.775938\n",
            "Time taken for 1 epoch 427.7195129394531 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.7639\n",
            "Epoch 3 Batch 100 Loss 0.7760\n",
            "Epoch 3 Batch 200 Loss 0.6795\n",
            "Epoch 3 Batch 300 Loss 0.7339\n",
            "Epoch 3 Batch 400 Loss 0.6436\n",
            "Epoch 3 Loss 0.710464\n",
            "Time taken for 1 epoch 446.94011759757996 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.7000\n",
            "Epoch 4 Batch 100 Loss 0.6170\n",
            "Epoch 4 Batch 200 Loss 0.6576\n",
            "Epoch 4 Batch 300 Loss 0.7046\n",
            "Epoch 4 Batch 400 Loss 0.6604\n",
            "Epoch 4 Loss 0.668746\n",
            "Time taken for 1 epoch 448.7081129550934 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.6978\n",
            "Epoch 5 Batch 100 Loss 0.7047\n",
            "Epoch 5 Batch 200 Loss 0.6620\n",
            "Epoch 5 Batch 300 Loss 0.6342\n",
            "Epoch 5 Batch 400 Loss 0.6705\n",
            "Epoch 5 Loss 0.635036\n",
            "Time taken for 1 epoch 447.3245141506195 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.6337\n",
            "Epoch 6 Batch 100 Loss 0.6659\n",
            "Epoch 6 Batch 200 Loss 0.6069\n",
            "Epoch 6 Batch 300 Loss 0.5878\n",
            "Epoch 6 Batch 400 Loss 0.6084\n",
            "Epoch 6 Loss 0.607508\n",
            "Time taken for 1 epoch 451.112459897995 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.6437\n",
            "Epoch 7 Batch 100 Loss 0.5476\n",
            "Epoch 7 Batch 200 Loss 0.5987\n",
            "Epoch 7 Batch 300 Loss 0.5371\n",
            "Epoch 7 Batch 400 Loss 0.6218\n",
            "Epoch 7 Loss 0.576477\n",
            "Time taken for 1 epoch 450.0085480213165 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.5758\n",
            "Epoch 8 Batch 100 Loss 0.5364\n",
            "Epoch 8 Batch 200 Loss 0.5344\n",
            "Epoch 8 Batch 300 Loss 0.5422\n",
            "Epoch 8 Batch 400 Loss 0.5496\n",
            "Epoch 8 Loss 0.547512\n",
            "Time taken for 1 epoch 451.6480243206024 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.5453\n",
            "Epoch 9 Batch 100 Loss 0.5042\n",
            "Epoch 9 Batch 200 Loss 0.5557\n",
            "Epoch 9 Batch 300 Loss 0.5015\n",
            "Epoch 9 Batch 400 Loss 0.4966\n",
            "Epoch 9 Loss 0.520604\n",
            "Time taken for 1 epoch 449.2629735469818 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.5035\n",
            "Epoch 10 Batch 100 Loss 0.5128\n",
            "Epoch 10 Batch 200 Loss 0.4837\n",
            "Epoch 10 Batch 300 Loss 0.4916\n",
            "Epoch 10 Batch 400 Loss 0.4728\n",
            "Epoch 10 Loss 0.493206\n",
            "Time taken for 1 epoch 452.38542008399963 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.5186\n",
            "Epoch 11 Batch 100 Loss 0.4611\n",
            "Epoch 11 Batch 200 Loss 0.4197\n",
            "Epoch 11 Batch 300 Loss 0.4666\n",
            "Epoch 11 Batch 400 Loss 0.4500\n",
            "Epoch 11 Loss 0.467924\n",
            "Time taken for 1 epoch 451.46762561798096 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.5059\n",
            "Epoch 12 Batch 100 Loss 0.4321\n",
            "Epoch 12 Batch 200 Loss 0.4532\n",
            "Epoch 12 Batch 300 Loss 0.4277\n",
            "Epoch 12 Batch 400 Loss 0.4140\n",
            "Epoch 12 Loss 0.439529\n",
            "Time taken for 1 epoch 452.7219934463501 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.4774\n",
            "Epoch 13 Batch 100 Loss 0.4678\n",
            "Epoch 13 Batch 200 Loss 0.4938\n",
            "Epoch 13 Batch 300 Loss 0.4422\n",
            "Epoch 13 Batch 400 Loss 0.3940\n",
            "Epoch 13 Loss 0.413745\n",
            "Time taken for 1 epoch 450.55485463142395 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.4270\n",
            "Epoch 14 Batch 100 Loss 0.4165\n",
            "Epoch 14 Batch 200 Loss 0.4044\n",
            "Epoch 14 Batch 300 Loss 0.3527\n",
            "Epoch 14 Batch 400 Loss 0.3690\n",
            "Epoch 14 Loss 0.391127\n",
            "Time taken for 1 epoch 451.0392520427704 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.4145\n",
            "Epoch 15 Batch 100 Loss 0.3786\n",
            "Epoch 15 Batch 200 Loss 0.3540\n",
            "Epoch 15 Batch 300 Loss 0.3649\n",
            "Epoch 15 Batch 400 Loss 0.3879\n",
            "Epoch 15 Loss 0.366132\n",
            "Time taken for 1 epoch 451.78452706336975 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.3518\n",
            "Epoch 16 Batch 100 Loss 0.3556\n",
            "Epoch 16 Batch 200 Loss 0.3253\n",
            "Epoch 16 Batch 300 Loss 0.3363\n",
            "Epoch 16 Batch 400 Loss 0.3254\n",
            "Epoch 16 Loss 0.345981\n",
            "Time taken for 1 epoch 451.3972201347351 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.3379\n",
            "Epoch 17 Batch 100 Loss 0.3205\n",
            "Epoch 17 Batch 200 Loss 0.3103\n",
            "Epoch 17 Batch 300 Loss 0.3275\n",
            "Epoch 17 Batch 400 Loss 0.3171\n",
            "Epoch 17 Loss 0.326346\n",
            "Time taken for 1 epoch 450.74470567703247 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.3316\n",
            "Epoch 18 Batch 100 Loss 0.3259\n",
            "Epoch 18 Batch 200 Loss 0.3062\n",
            "Epoch 18 Batch 300 Loss 0.2875\n",
            "Epoch 18 Batch 400 Loss 0.3013\n",
            "Epoch 18 Loss 0.310747\n",
            "Time taken for 1 epoch 452.2551794052124 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.3197\n",
            "Epoch 19 Batch 100 Loss 0.3112\n",
            "Epoch 19 Batch 200 Loss 0.2535\n",
            "Epoch 19 Batch 300 Loss 0.2557\n",
            "Epoch 19 Batch 400 Loss 0.2411\n",
            "Epoch 19 Loss 0.291389\n",
            "Time taken for 1 epoch 451.4141526222229 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.3106\n",
            "Epoch 20 Batch 100 Loss 0.2857\n",
            "Epoch 20 Batch 200 Loss 0.2601\n",
            "Epoch 20 Batch 300 Loss 0.2655\n",
            "Epoch 20 Batch 400 Loss 0.2848\n",
            "Epoch 20 Loss 0.273247\n",
            "Time taken for 1 epoch 450.4121530056 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOl6jMUBXJEY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(image):\n",
        "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
        "\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "\n",
        "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
        "\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result, attention_plot\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    attention_plot = attention_plot[:len(result), :]\n",
        "    return result, attention_plot\n",
        "\n",
        "\n",
        "def evaluate_without_plot(image):\n",
        "    hidden = decoder.reset_state(batch_size=1)\n",
        "\n",
        "    temp_input = tf.expand_dims(load_image(image)[0], 0)\n",
        "    img_tensor_val = image_features_extract_model(temp_input)\n",
        "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
        "\n",
        "    features = encoder(img_tensor_val)\n",
        "\n",
        "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
        "    result = []\n",
        "\n",
        "    for i in range(max_length):\n",
        "        predictions, hidden, attention_weights = decoder(dec_input, features, hidden)\n",
        "        predicted_id = tf.random.categorical(predictions, 1)[0][0].numpy()\n",
        "        result.append(tokenizer.index_word[predicted_id])\n",
        "\n",
        "        if tokenizer.index_word[predicted_id] == '<end>':\n",
        "            return result\n",
        "\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4momHmwXUOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_captions = []\n",
        "pred_captions = []\n",
        "\n",
        "with open('/content/drive/My Drive/Google TC/all_captions.csv', 'w') as file:\n",
        "    writer = csv.writer(file)\n",
        "    writer.writerow([\"true_caption\", \"pred_caption\"])\n",
        "\n",
        "    for idx in range(len(img_name_val)):\n",
        "        r_cap = [tokenizer.index_word[i] for i in cap_val[idx] if i not in [0]][1:-1]\n",
        "        p_cap = evaluate_without_plot(img_name_val[idx])[:-1]\n",
        "    \n",
        "        real_captions.append(r_cap)\n",
        "        pred_captions.append(p_cap)\n",
        "\n",
        "        writer.writerow([' '.join(r_cap), ' '.join(p_cap)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKXrgCA6XYbm",
        "colab_type": "text"
      },
      "source": [
        "# PART 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN6Hz9N-7SNm",
        "colab_type": "text"
      },
      "source": [
        "In this part of the code we import the captions file  from the first part and work on the different embedding and similarity techniques.\n",
        "First we try a $\\texttt{tfidf}$ embedding and then different kinds of sentence embeddings. \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJfy-gmoUrwJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import csv\n",
        "import cv2"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QVhhTRyXcJc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import the captions\n",
        "all_captions = pd.read_csv(\"/content/drive/My Drive/Google TC/all_captions.csv\", sep=',') \n",
        "\n",
        "# nested list of captions (list of words lists)\n",
        "real_captions = [x.split() for x in all_captions['true_caption'].tolist()]\n",
        "pred_captions = [x.split() for x in all_captions['pred_caption'].tolist()]\n",
        "\n",
        "# list of captions (list of strings)\n",
        "pred_captions_list = list(all_captions['pred_caption'].values)\n",
        "real_captions_list = list(all_captions['true_caption'].values)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugjk0HGGDSsT",
        "colab_type": "text"
      },
      "source": [
        "#### TFIDF and cosine similarity\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2YXhQJm5MIz",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing step that lemmatize the words of the captions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oztUjwWUdhd6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        },
        "outputId": "ca7aeef3-3940-4eb0-c170-c78b50952930"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "# Lemmatize with POS Tag\n",
        "\n",
        "def get_wordnet_pos(word):\n",
        "\n",
        "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
        "    tag_dict = {\"J\": wordnet.ADJ,\n",
        "                \"N\": wordnet.NOUN,\n",
        "                \"V\": wordnet.VERB,\n",
        "                \"R\": wordnet.ADV}\n",
        "\n",
        "    return tag_dict.get(tag, wordnet.NOUN)\n",
        "\n",
        "def clean_text(sentence):\n",
        "\n",
        "  words = word_tokenize(sentence)\n",
        "  clean = ' '.join([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words])\n",
        "\n",
        "  return clean\n",
        "\n",
        "\n",
        "for i in range(len(pred_captions_list)):\n",
        "  sentence = pred_captions_list[i]\n",
        "  new = clean_text(sentence)\n",
        "  pred_captions_list[i] = new\n",
        "for i in range(len(real_captions_list)):\n",
        "  sentence = real_captions_list[i]\n",
        "  new = clean_text(sentence)\n",
        "  real_captions_list[i] = new"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGalU5UW5lqZ",
        "colab_type": "text"
      },
      "source": [
        "TfidfVectorizer transforms the captions to the tfidf embedding space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUgnJNC0iFdU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "word_vect = TfidfVectorizer(stop_words='english', strip_accents='ascii',sublinear_tf=True)\n",
        "word_vect.fit(pred_captions_list)\n",
        "\n",
        "pred_tfidf = word_vect.transform(pred_captions_list)\n",
        "real_tfidf = word_vect.transform(real_captions_list)\n",
        "\n",
        "pred_tfidf = pred_tfidf.toarray()\n",
        "real_tfidf = real_tfidf.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmR7S8qQ5su9",
        "colab_type": "text"
      },
      "source": [
        "Function that computes the cosine similarity, dot product only since the vectors are normalized already"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IhyfhwWZ9eX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tfidf_score(idx, real_tfidf, pred_tfidf):\n",
        "    target = real_tfidf[idx]\n",
        "    idx_score = []\n",
        "    for idx in range(1000):\n",
        "        idx_score.append((idx, np.dot(target, pred_tfidf[idx])))\n",
        "    idx_score.sort(key=lambda x: x[1], reverse=True)\n",
        "    return idx_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2rB33L2Dg72",
        "colab_type": "text"
      },
      "source": [
        "#### Word2Vec and WMD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOwt9Hw055Tp",
        "colab_type": "text"
      },
      "source": [
        "Download the model and the nltk stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0XeVRG6DZhE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "87bfee4e-5222-4e92-fac1-f3515205f0e8"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk import download\n",
        "import gensim.downloader as api\n",
        "\n",
        "model = api.load('word2vec-google-news-300')\n",
        "download('stopwords')  # Download stopwords list.\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnehr5ML5-Eh",
        "colab_type": "text"
      },
      "source": [
        "Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJVmzMSqDqjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_captions_list_sw = []\n",
        "real_captions_list_sw = []\n",
        "\n",
        "for i in range(len(pred_captions_list)):\n",
        "    pred_captions_list_sw.append([w for w in pred_captions_list[i].lower().split() if w not in stop_words])\n",
        "    real_captions_list_sw.append([w for w in real_captions_list[i].lower().split() if w not in stop_words])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehzo2Ljd6IHh",
        "colab_type": "text"
      },
      "source": [
        "Function that returns the dissimilarity based on the wm distance "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-FKPP26GxXo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.init_sims(replace=True) \n",
        "def wmd_score(idx, real_captions, pred_captions):\n",
        "    target = real_captions[idx]\n",
        "    idx_score = []\n",
        "    for idx in range(1000):\n",
        "        idx_score.append((idx, model.wmdistance(target, pred_captions[idx])))\n",
        "    idx_score.sort(key=lambda x: x[1])\n",
        "    return idx_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsE7-kByZBmQ",
        "colab_type": "text"
      },
      "source": [
        "#### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zfG2qs1ZS-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy09eZ8VZKyX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "08edb38a-2b68-4baa-f04c-df54496f7948"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 405M/405M [00:05<00:00, 78.7MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQTLuOlN6TaJ",
        "colab_type": "text"
      },
      "source": [
        "Encode the captions using BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHi1zYvxZdW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_bert = sbert_model.encode(pred_captions_list)\n",
        "real_bert = sbert_model.encode(real_captions_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe8TJ_816Wl4",
        "colab_type": "text"
      },
      "source": [
        "Functions that return the score given by the cosine similarity or the l2 norm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geq9LKheZvrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cos_sim(u, v):\n",
        "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
        "def l2_norm(u,v):\n",
        "    return np.linalg.norm(u-v)\n",
        "def bert_score(idx, real_captions, pred_captions):\n",
        "    target = real_captions[idx]\n",
        "    idx_score = []\n",
        "    for idx in range(1000):\n",
        "        idx_score.append((idx, cos_sim(target, pred_captions[idx])))\n",
        "    idx_score.sort(key=lambda x: x[1], reverse=True)\n",
        "    return idx_score"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEygldmIiiwL",
        "colab_type": "text"
      },
      "source": [
        "#### USE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahHfFs-oiiSs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" \n",
        "model = hub.load(module_url)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvSREp5EJVuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# text preprocessing: remowe stopwords and stemming\n",
        "\n",
        "'''\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def string_processing (string):\n",
        "    return stemmer.stem(' '.join([word for word in string.split() if word not in stop_words]))\n",
        "\n",
        "for i in range(len(pred_captions_list)):\n",
        "    pred_captions_list[i] = string_processing(pred_captions_list[i])\n",
        "\n",
        "for i in range(len(real_captions_list)):\n",
        "    real_captions_list[i] = string_processing(real_captions_list[i])\n",
        "\n",
        "'''"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax0t8Io86dMb",
        "colab_type": "text"
      },
      "source": [
        "Embedding of the captions with USE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-mC32LqjCwP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred_use = model(pred_captions_list)\n",
        "real_use = model(real_captions_list)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38_np_Qb6g0n",
        "colab_type": "text"
      },
      "source": [
        "Functions that compute the scores based on cosine similarity \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJosxINdj0dK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def use_score(idx, real_captions, pred_captions):\n",
        "    target = real_captions[idx]\n",
        "    idx_score = []\n",
        "    for idx in range(1000):\n",
        "        idx_score.append((idx, cos_sim(target, pred_captions[idx])))\n",
        "    idx_score.sort(key=lambda x: x[1], reverse=True)\n",
        "    return idx_score\n",
        "def use_score_euc(idx, real_captions, pred_captions):\n",
        "    target = real_captions[idx]\n",
        "    idx_score = []\n",
        "    for idx in range(1000):\n",
        "        idx_score.append((idx, l2_norm(target, pred_captions[idx])))\n",
        "    idx_score.sort(key=lambda x: x[1])\n",
        "    return idx_score   "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j37Z9yqcOKe",
        "colab_type": "text"
      },
      "source": [
        "#### SUBMISSION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leI6c8RY6qCC",
        "colab_type": "text"
      },
      "source": [
        "Function that creates the submission file. In the code it is necessary to specify the score used. In this case we are using $\\texttt{use_score_euc}$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6_QFYN2cJh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_submission_file(real_captions, pred_captions):\n",
        "\n",
        "    with open('/content/drive/My Drive/Google TC/submission_use_stem.csv', 'w') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerow([\"caption\", \"image_list\"])\n",
        "\n",
        "        for idx in range(1000):\n",
        "\n",
        "            b_score_res = use_score(idx, real_use, pred_use)\n",
        "\n",
        "            writer.writerow([' '.join(real_captions[idx]), ' '.join(list(map(lambda x: str(x[0]), b_score_res[:5000])))])\n",
        "            if (idx+1) % 100 ==0:\n",
        "                print('rows written ',idx)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVRb0EipcI-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "create_submission_file(real_captions, pred_captions)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}